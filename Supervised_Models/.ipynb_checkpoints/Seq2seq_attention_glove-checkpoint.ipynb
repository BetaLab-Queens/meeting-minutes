{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention seq2seq using GloVe embeddings, not working yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "from attention import AttentionLayer\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"..\\\\Data\\\\model_training\\\\100000_samples_Reviews.csv\")\n",
    "data.drop_duplicates(subset=['Text'],inplace=True)  #dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)#dropping na rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labr...</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with ...</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The fl...</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      Text  \\\n",
       "0  I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labr...   \n",
       "1           Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".   \n",
       "2  This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with ...   \n",
       "3  If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The fl...   \n",
       "4                                                             Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.   \n",
       "\n",
       "                 Summary  \n",
       "0  Good Quality Dog Food  \n",
       "1      Not as Advertised  \n",
       "2  \"Delight\" says it all  \n",
       "3         Cough Medicine  \n",
       "4            Great taffy  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data=data.rename(columns = {'article': 'Text', 'title': 'Summary'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, exclude_encodings=\"lxml\").text # removes html/xml taggs\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(text_cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString\n",
    "\n",
    "#Call the above function\n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(summary_cleaner(t))\n",
    "\n",
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Summary: good quality dog food \n",
      "\n",
      "Review: product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Summary: not as advertised \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"Review:\",data['cleaned_text'][i])\n",
    "    print(\"Summary:\",data['cleaned_summary'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text=30\n",
    "max_len_summary=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_len_summary and len(cleaned_text[i].split())<=max_len_text):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better</td>\n",
       "      <td>sostok good quality dog food  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo</td>\n",
       "      <td>sostok not as advertised  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal</td>\n",
       "      <td>sostok cough medicine  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great taffy great price wide assortment yummy taffy delivery quick taffy lover deal</td>\n",
       "      <td>sostok great taffy  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saltwater taffy great flavors soft chewy candy individually wrapped well none candies stuck together happen expensive version fralinger would highly recommend candy served beach themed party every...</td>\n",
       "      <td>sostok great just as good as the expensive brands  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52254</th>\n",
       "      <td>favorite coffee also best price found thanks amazon com</td>\n",
       "      <td>sostok great tasting coffee  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52255</th>\n",
       "      <td>generally like millstone coffees one major disappointment weak flavor taste drinker weak non acidic coffee might find liking</td>\n",
       "      <td>sostok millstone kona bad blend  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52256</th>\n",
       "      <td>disappointed trying use mixing fresh roast beans grinding seem like old stock beans practically dessicated fresh roasted sheen bean speak lot like typical pack hotel coffee would recommend reorder</td>\n",
       "      <td>sostok not very fresh beans are very dry  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52257</th>\n",
       "      <td>enjoy blend fan starbuck coffee enjoy looking lighter roast good price flavor simply worst kona blend ever tried maybe taste different moving different blend</td>\n",
       "      <td>sostok really did not like this one  eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52258</th>\n",
       "      <td>sale thought would try much surprise whole family loved strong perfect flavor reminder kona coffee tasted hawaii vacation</td>\n",
       "      <td>sostok like coffee in star hotel  eostok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52259 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          text  \\\n",
       "0                                         bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better   \n",
       "1                                                                        product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo   \n",
       "2                                                                                  looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal   \n",
       "3                                                                                                                          great taffy great price wide assortment yummy taffy delivery quick taffy lover deal   \n",
       "4      saltwater taffy great flavors soft chewy candy individually wrapped well none candies stuck together happen expensive version fralinger would highly recommend candy served beach themed party every...   \n",
       "...                                                                                                                                                                                                        ...   \n",
       "52254                                                                                                                                                  favorite coffee also best price found thanks amazon com   \n",
       "52255                                                                             generally like millstone coffees one major disappointment weak flavor taste drinker weak non acidic coffee might find liking   \n",
       "52256     disappointed trying use mixing fresh roast beans grinding seem like old stock beans practically dessicated fresh roasted sheen bean speak lot like typical pack hotel coffee would recommend reorder   \n",
       "52257                                            enjoy blend fan starbuck coffee enjoy looking lighter roast good price flavor simply worst kona blend ever tried maybe taste different moving different blend   \n",
       "52258                                                                                sale thought would try much surprise whole family loved strong perfect flavor reminder kona coffee tasted hawaii vacation   \n",
       "\n",
       "                                                         summary  \n",
       "0                           sostok good quality dog food  eostok  \n",
       "1                               sostok not as advertised  eostok  \n",
       "2                                  sostok cough medicine  eostok  \n",
       "3                                     sostok great taffy  eostok  \n",
       "4      sostok great just as good as the expensive brands  eostok  \n",
       "...                                                          ...  \n",
       "52254                        sostok great tasting coffee  eostok  \n",
       "52255                    sostok millstone kona bad blend  eostok  \n",
       "52256           sostok not very fresh beans are very dry  eostok  \n",
       "52257                sostok really did not like this one  eostok  \n",
       "52258                   sostok like coffee in star hotel  eostok  \n",
       "\n",
       "[52259 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply start of sentence/ end of sentence tokens to summary, used for \n",
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sostok great coffee  eostok',\n",
       "       'sostok iby far the best gluten free pasta  eostok',\n",
       "       'sostok misleading  eostok', 'sostok quality tea  eostok',\n",
       "       'sostok good coffee  eostok',\n",
       "       'sostok my favorite cup tea blend  eostok',\n",
       "       'sostok good food  eostok', 'sostok great pancakes  eostok',\n",
       "       'sostok light and tasty  eostok', 'sostok powder cereal  eostok'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X: Text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 66.13882696594665\n",
      "Total Coverage of rare words: 2.7712041060446184\n"
     ]
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "thresh=4 # threshold = 4. Word whose count is below 4 is considered as a rare word\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8810"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_cnt-cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8811"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) # for the vocabulary\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_len_text, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_len_text, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "x_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 504,  112,   86, ...,    0,    0,    0],\n",
       "       [  74,  153, 2113, ...,  140,   72,   87],\n",
       "       [ 150,    2,   68, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 454,  573,  122, ...,    0,    0,    0],\n",
       "       [   6,  346,  953, ...,    0,    0,    0],\n",
       "       [  12,   50,  586, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y: Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 77.7399029025927\n",
      "Total Coverage of rare words: 5.128902602411595\n"
     ]
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "thresh=6 # threshold = 6. Word whose count is below 6 is considered as a rare word, since summaries are less.\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2155"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_cnt-cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_len_summary, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1\n",
    "y_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok']-len(y_tr) # should be 0, check if there are any sostok's in the word list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Creating embeddings_index took 12.4 seconds.\n"
     ]
    }
   ],
   "source": [
    "from load_glove_embeddings import load_glove_embeddings # local function from .py file\n",
    "glove_dimension = 50\n",
    "import time\n",
    "timer_start = time.time()\n",
    "# word2index, embedding_matrix = load_glove_embeddings('Data/word_embeddings/glove.6B.'+str(glove_dimension)+'d.txt', embedding_dim=glove_dimension, include_empty_char=False)\n",
    "word2index, embedding_matrix = load_glove_embeddings('../Data/word_embeddings/glove.6B.'+str(glove_dimension)+'d.txt', embedding_dim=glove_dimension, include_empty_char=False)\n",
    "\n",
    "print('Found %s word vectors.' % len(word2index))\n",
    "print(\"Creating embeddings_index took\", round(time.time() - timer_start, 1), \"seconds.\")\n",
    "del timer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
       "        -0.11514   , -0.78580999],\n",
       "       [ 0.013441  ,  0.23682   , -0.16899   , ..., -0.56656998,\n",
       "         0.044691  ,  0.30392   ],\n",
       "       [ 0.15164   ,  0.30177   , -0.16763   , ..., -0.35652   ,\n",
       "         0.016413  ,  0.10216   ],\n",
       "       ...,\n",
       "       [-0.51181   ,  0.058706  ,  1.09130001, ..., -0.25003001,\n",
       "        -1.125     ,  1.58630002],\n",
       "       [-0.75897998, -0.47426   ,  0.47369999, ...,  0.78953999,\n",
       "        -0.014116  ,  0.64480001],\n",
       "       [ 0.072617  , -0.51393002,  0.47279999, ..., -0.18907   ,\n",
       "        -0.59021002,  0.55558997]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " (',', 1),\n",
       " ('.', 2),\n",
       " ('of', 3),\n",
       " ('to', 4),\n",
       " ('and', 5),\n",
       " ('in', 6),\n",
       " ('a', 7),\n",
       " ('\"', 8),\n",
       " (\"'s\", 9)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(word2index.items(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix_creater(embedding_dimention, word_index): \n",
    "    \"\"\"\n",
    "    Uses GloVe as a global word embedding. \n",
    "    \n",
    "    embedding_dimention: usually in the title of glove.6D.'embedding_dimention', in this notebook use 50. \n",
    "    word_index: the input word embeddings\n",
    "    \n",
    "    returns: a local embedding matrix, to be input as weights [embedding] for the constructor\n",
    "        for keras' Embedding object. \n",
    "    \"\"\"\n",
    "    local_embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = word2index.get(word) # from cell above, from glove.\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "            local_embedding_matrix[i] = embedding_vector\n",
    "    return local_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26018"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 1, 'good': 2, 'like': 3, 'love': 4, 'taste': 5}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(islice(x_tokenizer.word_index.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tokenizer.num_words: 8810\n",
      "x_voc: 8811\n"
     ]
    }
   ],
   "source": [
    "print('x_tokenizer.num_words:', x_tokenizer.num_words)\n",
    "print('x_voc:', x_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8812, 50)\n",
      "(2157, 50)\n"
     ]
    }
   ],
   "source": [
    "text_embedding_matrix = embedding_matrix_creater(glove_dimension, \n",
    "        word_index=dict(islice(x_tokenizer.word_index.items(), x_tokenizer.num_words+1)))\n",
    "print(text_embedding_matrix.shape)\n",
    "\n",
    "sum_embedding_matrix = embedding_matrix_creater(glove_dimension, \n",
    "        word_index=dict(islice(y_tokenizer.word_index.items(), y_tokenizer.num_words+1)))\n",
    "print(sum_embedding_matrix.shape)\n",
    "\n",
    "encoder_embedding_layer = Embedding(input_dim = int(text_embedding_matrix.shape[0]), # vocab size\n",
    "                                    output_dim = int(text_embedding_matrix.shape[1]), # embedding dimension\n",
    "                                    input_length = max_len_text, # sequence length\n",
    "                                    weights = [text_embedding_matrix],\n",
    "                                    trainable = False) # False because these embeddings have already been trained. \n",
    "\n",
    "decoder_embedding_layer = Embedding(input_dim = int(sum_embedding_matrix.shape[0]),\n",
    "                                    output_dim = int(sum_embedding_matrix.shape[1]),\n",
    "                                    input_length = max_len_summary,\n",
    "                                    weights = [sum_embedding_matrix],\n",
    "                                    trainable = False) # False because these embeddings have already been trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 50)       440600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 30, 300), (N 421200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 30, 300), (N 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     107850      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 30, 300), (N 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  421200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 2157)   1296357     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,309,907\n",
      "Trainable params: 3,761,457\n",
      "Non-trainable params: 548,450\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# (number of inputs, length of the output sentence, the number of words in the output)\n",
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "latent_dim = 300\n",
    "# embedding_dim=100 # OLD, can be glove dimension now\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_len_text,))\n",
    "\n",
    "#embedding layer\n",
    "# enc_emb =  Embedding(x_voc_size, embedding_dim,trainable=True)(encoder_inputs) # OLD\n",
    "enc_emb = encoder_embedding_layer(encoder_inputs) # NEW\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "# dec_emb_layer = Embedding(y_voc_size, embedding_dim,trainable=True) # OLD\n",
    "# dec_emb = dec_emb_layer(decoder_inputs) # OLD\n",
    "dec_emb = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(int(sum_embedding_matrix.shape[0]), activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5100, 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45840 samples, validate on 5100 samples\n",
      "Epoch 1/2\n",
      "45840/45840 [==============================] - 673s 15ms/sample - loss: 2.8551 - val_loss: 2.7940\n",
      "Epoch 2/2\n",
      "45840/45840 [==============================] - 629s 14ms/sample - loss: 2.7592 - val_loss: 2.8312\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
    "history=model.fit([x_tr,y_tr[:,:-1]], \n",
    "                  y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:],\n",
    "                  epochs=2,\n",
    "                  callbacks=[es],\n",
    "                  batch_size=128,\n",
    "                  validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferernce, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word \n",
    "reverse_source_word_index=x_tokenizer.index_word \n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= decoder_embedding_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    print('e_out:', e_out[0][0][0])\n",
    "    print('e_h:', e_h[0][0])\n",
    "    print('e_c:', e_c[0][0])\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "    # works fine\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "#         print(\"output_tokens:\", output_tokens)\n",
    "    \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         print(sampled_token_index)\n",
    "#         if sampled_token_index != 0: # loops here too many times, shouldn't have an index of 0, but consistently does. \n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        print('sample token:', sampled_token)\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: oatmeal old hard parts bad taste throw away never order \n",
      "Original summary: bad \n",
      "\n",
      "e_out: -0.0032754496\n",
      "e_h: -0.033236608\n",
      "e_c: -0.12873837\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: always thought lentils beans old people love amy kitchen soups wanted try good thing love flavor perfect consistency perfect tastes excellent sometimes even squirt lime juice spoon eating mix bit \n",
      "Original summary: lentils are not just for old people \n",
      "\n",
      "e_out: -0.0031595416\n",
      "e_h: 0.0023712527\n",
      "e_c: 0.020420209\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: pretty good stuff found cheaper local grocery store buying amazon best canned chowder ever purchased nothing good home made works quick snack \n",
      "Original summary: clam chowder \n",
      "\n",
      "e_out: -0.0032607948\n",
      "e_h: -0.006283649\n",
      "e_c: -0.034992136\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: good flavor texture taste price nephew enjoys sharing uses selection pack single wrapped \n",
      "Original summary: needed \n",
      "\n",
      "e_out: -0.0029762867\n",
      "e_h: -0.03068973\n",
      "e_c: -0.12206786\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: get bags bag pretzels taste worth dollar bag \n",
      "Original summary: not worth the money \n",
      "\n",
      "e_out: -0.0029547547\n",
      "e_h: -0.03392267\n",
      "e_c: -0.13032946\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: awesome flavor gives fix chocolate ruin diet highly recommend flavor glad bought lot \n",
      "Original summary: chocolate fix \n",
      "\n",
      "e_out: -0.0032754496\n",
      "e_h: -0.030696528\n",
      "e_c: -0.12209007\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: appear first glance similar butters taste however lighter delicate infused hazelnut air cookie dip coffee becomes saturated rather quickly cookie mouth sweet center taking center stage nuff said \n",
      "Original summary: good with coffee \n",
      "\n",
      "e_out: -0.0032519118\n",
      "e_h: 0.00040549578\n",
      "e_c: 0.0029925774\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: light medium roast wish slightly stronger flavor good every morning using cups pumps make strong possible \n",
      "Original summary: like it \n",
      "\n",
      "e_out: -0.0031930082\n",
      "e_h: -0.02350571\n",
      "e_c: -0.10170062\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: soy corn gluten allergies product gives strong allergic reaction get allergies unsafe use people allergies may good product \n",
      "Original summary: tastes good but is \n",
      "\n",
      "e_out: -0.0032754496\n",
      "e_h: -0.01643499\n",
      "e_c: -0.07938044\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: flavor partially saved added sugar adds get stuck teeth got steep discount still would buy price point \n",
      "Original summary: by syrup \n",
      "\n",
      "e_out: -0.003275448\n",
      "e_h: -0.019789545\n",
      "e_c: -0.09053044\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: daughter loves think eat though especially since pack boxes \n",
      "Original summary: great organic cookies \n",
      "\n",
      "e_out: -0.0032190979\n",
      "e_h: -0.033634316\n",
      "e_c: -0.1296995\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: delightfully tasting onions makes difference vodka used regular onions bartender make one onions showed difference onion make glad walked bar \n",
      "Original summary: delicious \n",
      "\n",
      "e_out: -0.0032754496\n",
      "e_h: -0.01110052\n",
      "e_c: -0.05718934\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: particularly picky dog loves good size give two three feel like getting treats hey gluten free \n",
      "Original summary: very good yum \n",
      "\n",
      "e_out: -0.0032174168\n",
      "e_h: -0.023483234\n",
      "e_c: -0.101628534\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: mean order pound bag wow lot food sure mix protein else dog suffer bit internal distress \n",
      "Original summary: original \n",
      "\n",
      "e_out: -0.0032539347\n",
      "e_h: -0.023540135\n",
      "e_c: -0.101804346\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: black tea strong priced reasonably though adding dash milk black tea drinking without milk sugar quite decent faint aroma taste strong smell good tazo awake nevertheless pretty good value get \n",
      "Original summary: not very strong black tea \n",
      "\n",
      "e_out: -0.0031190729\n",
      "e_h: 0.0024044893\n",
      "e_c: 0.02054588\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: best ketchup ever tasted although expensive ketchup brands raise bar various brands eating years ketchup thick taste great bottled product processed holds excellent believer organicville brands \n",
      "Original summary: best ketchup around \n",
      "\n",
      "e_out: -0.002998013\n",
      "e_h: -0.0016834356\n",
      "e_c: -0.010953579\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: mood good eats try jalapeno pretzels good hard great really true get packages \n",
      "Original summary: soooo gooood \n",
      "\n",
      "e_out: -0.0032753956\n",
      "e_h: -0.030685993\n",
      "e_c: -0.12205812\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: bought product many months ago loved longer buying brand doubled price bought originally buy another brand \n",
      "Original summary: the price doubled \n",
      "\n",
      "e_out: -0.0032564967\n",
      "e_h: -0.02350831\n",
      "e_c: -0.101710245\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: drink disaster taste pina colada mix bad thought expired using splenda bad idea bob read reviews recipe gotta sugarless try stevia much smoother \n",
      "Original summary: worst drink mix ever \n",
      "\n",
      "e_out: -0.0032753304\n",
      "e_h: -0.0045014718\n",
      "e_c: -0.026450606\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n",
      "Review: far tried plain yogurt amazing strawberry tasting right amount sweet started slowly putting half recommended added little little strong start low add desired taste \n",
      "Original summary: awesome \n",
      "\n",
      "e_out: -0.0030638548\n",
      "e_h: -0.0031606571\n",
      "e_c: -0.019408043\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print()\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_len_text)))\n",
    "    print(\"________________________________________________________\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 504  112   86 2538  113    5  618  224   84   36    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[  74  153 2113  334  112  218    4 2773  875  836  298   35    2  175\n",
      "    4    8   52  606   52   39   70  429   38 4711  884  327 1416  140\n",
      "   72   87]\n",
      "[ 150    2   68   42  207  108  156   46   92   17   12  550 3077   54\n",
      "  127  244    2  217   59  219  208   57    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(x_tr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 73  2  0  0  0  0  0]\n",
      "[1742   49    9   45    7  170  574    2]\n",
      "[   1 1915 1743    2    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(y_tr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model.\n",
    "# model_json = model.to_json()\n",
    "# with open('model_v1.json',\"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# model.save_weights('model_v1.h5')\n",
    "# print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: may be unable to load model since a custom \"attention layer\" is used. \n",
    "# from keras.models import model_from_json\n",
    "# # load json and create model\n",
    "# json_file = open('model_v1.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model_v1.h5\")\n",
    "# print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_state_input_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tr[i].reshape(1,max_len_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
