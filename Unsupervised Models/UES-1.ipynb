{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import gensim\n",
    "import skipthoughts\n",
    "import re           \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = ['.', ',', '!', '?', ';', ':']\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_text = [w for w in text.split() if not w in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    filtered_text = ''.join([w for w in text if w not in punctuation])\n",
    "    return filtered_text\n",
    "\n",
    "def map_contractions(text):\n",
    "    filtered_text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return lemmatized_text\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stemmed_text = PorterStemmer(text)\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = text.lower()\n",
    "    cleaned_text = map_contractions(cleaned_text)\n",
    "    #cleaned_text = word_stemmer(cleaned_text)\n",
    "    #cleaned_text = word_lemmatizer(cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"She reached her goal, exhausted. Even more chilling to her was that the euphoria that she thought she'd feel upon reaching it wasn't there. Something wasn't right. Was this the only feeling she'd have for over five years of hard work?\"\n",
    "cleaned_text = clean_text(doc)\n",
    "cleaned_sentences = sent_tokenize(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she reached her goal, exhausted.',\n",
       " 'even more chilling to her wa that the euphoria that she thought she would feel upon reaching it wa not there.',\n",
       " 'something wa not right.',\n",
       " 'wa this the only feeling she would have for over five year of hard work?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-28 17:41:26,508 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "C:\\Users\\Felix\\Anaconda3\\envs\\betaLab\\lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-28 17:41:26,584 : INFO : collecting all words and their counts\n",
      "2020-05-28 17:41:26,590 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-05-28 17:41:26,663 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "2020-05-28 17:41:26,669 : INFO : Loading a fresh vocabulary\n",
      "2020-05-28 17:41:26,719 : INFO : effective_min_count=2 retains 3955 unique words (56% of original 6981, drops 3026)\n",
      "2020-05-28 17:41:26,726 : INFO : effective_min_count=2 leaves 55126 word corpus (94% of original 58152, drops 3026)\n",
      "2020-05-28 17:41:26,778 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2020-05-28 17:41:26,781 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2020-05-28 17:41:26,786 : INFO : downsampling leaves estimated 42390 word corpus (76.9% of prior 55126)\n",
      "2020-05-28 17:41:26,819 : INFO : estimated required memory for 3955 words and 50 dimensions: 3619500 bytes\n",
      "2020-05-28 17:41:26,823 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-28 17:41:28,963 : INFO : training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-05-28 17:42:03,271 : INFO : EPOCH 1 - PROGRESS: at 16.67% examples, 205 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:42:06,026 : INFO : EPOCH 1 - PROGRESS: at 32.00% examples, 389 words/s, in_qsize 4, out_qsize 0\n",
      "2020-05-28 17:42:42,676 : INFO : EPOCH 1 - PROGRESS: at 66.67% examples, 387 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:42:42,684 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:42:45,164 : INFO : EPOCH 1 - PROGRESS: at 83.00% examples, 464 words/s, in_qsize 1, out_qsize 1\n",
      "2020-05-28 17:42:45,176 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:42:45,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:42:45,284 : INFO : EPOCH - 1 : training on 58152 raw words (42426 effective words) took 76.3s, 556 effective words/s\n",
      "2020-05-28 17:43:27,766 : INFO : EPOCH 2 - PROGRESS: at 16.67% examples, 166 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:44:02,387 : INFO : EPOCH 2 - PROGRESS: at 67.00% examples, 369 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:44:02,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:44:03,289 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:44:03,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:44:03,347 : INFO : EPOCH - 2 : training on 58152 raw words (42428 effective words) took 78.1s, 544 effective words/s\n",
      "2020-05-28 17:44:38,393 : INFO : EPOCH 3 - PROGRESS: at 16.67% examples, 202 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:44:39,646 : INFO : EPOCH 3 - PROGRESS: at 35.33% examples, 393 words/s, in_qsize 4, out_qsize 0\n",
      "2020-05-28 17:44:41,089 : INFO : EPOCH 3 - PROGRESS: at 50.67% examples, 572 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:45:12,579 : INFO : EPOCH 3 - PROGRESS: at 66.67% examples, 413 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:45:12,598 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:45:14,558 : INFO : EPOCH 3 - PROGRESS: at 83.67% examples, 501 words/s, in_qsize 1, out_qsize 1\n",
      "2020-05-28 17:45:14,565 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:45:14,639 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:45:14,644 : INFO : EPOCH - 3 : training on 58152 raw words (42471 effective words) took 71.3s, 596 effective words/s\n",
      "2020-05-28 17:45:50,380 : INFO : EPOCH 4 - PROGRESS: at 16.67% examples, 197 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:45:52,201 : INFO : EPOCH 4 - PROGRESS: at 50.67% examples, 574 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:46:21,760 : INFO : EPOCH 4 - PROGRESS: at 66.67% examples, 425 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:46:21,780 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:46:25,323 : INFO : EPOCH 4 - PROGRESS: at 83.00% examples, 500 words/s, in_qsize 1, out_qsize 1\n",
      "2020-05-28 17:46:25,329 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:46:25,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:46:25,358 : INFO : EPOCH - 4 : training on 58152 raw words (42442 effective words) took 70.7s, 600 effective words/s\n",
      "2020-05-28 17:46:51,071 : INFO : EPOCH 5 - PROGRESS: at 16.67% examples, 274 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:47:17,671 : INFO : EPOCH 5 - PROGRESS: at 51.33% examples, 405 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:47:20,102 : INFO : EPOCH 5 - PROGRESS: at 68.33% examples, 515 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:47:20,112 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:47:22,915 : INFO : EPOCH 5 - PROGRESS: at 83.67% examples, 618 words/s, in_qsize 1, out_qsize 1\n",
      "2020-05-28 17:47:22,919 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:47:28,734 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 669 words/s, in_qsize 0, out_qsize 1\n",
      "2020-05-28 17:47:28,736 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:47:28,736 : INFO : EPOCH - 5 : training on 58152 raw words (42383 effective words) took 63.4s, 669 effective words/s\n",
      "2020-05-28 17:48:05,487 : INFO : EPOCH 6 - PROGRESS: at 16.67% examples, 192 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:48:40,731 : INFO : EPOCH 6 - PROGRESS: at 67.00% examples, 393 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:48:40,737 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:48:41,418 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:48:41,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:48:41,492 : INFO : EPOCH - 6 : training on 58152 raw words (42320 effective words) took 72.7s, 582 effective words/s\n",
      "2020-05-28 17:49:16,097 : INFO : EPOCH 7 - PROGRESS: at 16.67% examples, 203 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:49:18,674 : INFO : EPOCH 7 - PROGRESS: at 35.33% examples, 382 words/s, in_qsize 4, out_qsize 0\n",
      "2020-05-28 17:49:19,722 : INFO : EPOCH 7 - PROGRESS: at 50.67% examples, 563 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:49:49,380 : INFO : EPOCH 7 - PROGRESS: at 67.00% examples, 417 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:49:49,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:49:49,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:49:51,170 : INFO : EPOCH 7 - PROGRESS: at 100.00% examples, 607 words/s, in_qsize 0, out_qsize 1\n",
      "2020-05-28 17:49:51,182 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:49:51,184 : INFO : EPOCH - 7 : training on 58152 raw words (42324 effective words) took 69.7s, 607 effective words/s\n",
      "2020-05-28 17:50:18,458 : INFO : EPOCH 8 - PROGRESS: at 16.67% examples, 258 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:50:44,011 : INFO : EPOCH 8 - PROGRESS: at 51.33% examples, 401 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:50:45,249 : INFO : EPOCH 8 - PROGRESS: at 68.33% examples, 522 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:50:45,249 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:50:46,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:50:51,765 : INFO : EPOCH 8 - PROGRESS: at 100.00% examples, 699 words/s, in_qsize 0, out_qsize 1\n",
      "2020-05-28 17:50:51,765 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:50:51,781 : INFO : EPOCH - 8 : training on 58152 raw words (42338 effective words) took 60.6s, 699 effective words/s\n",
      "2020-05-28 17:51:19,397 : INFO : EPOCH 9 - PROGRESS: at 18.67% examples, 258 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:51:43,477 : INFO : EPOCH 9 - PROGRESS: at 50.67% examples, 416 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:51:45,869 : INFO : EPOCH 9 - PROGRESS: at 66.67% examples, 527 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:51:45,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:51:46,059 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:51:52,147 : INFO : EPOCH 9 - PROGRESS: at 100.00% examples, 701 words/s, in_qsize 0, out_qsize 1\n",
      "2020-05-28 17:51:52,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:51:52,152 : INFO : EPOCH - 9 : training on 58152 raw words (42292 effective words) took 60.4s, 701 effective words/s\n",
      "2020-05-28 17:52:18,431 : INFO : EPOCH 10 - PROGRESS: at 16.67% examples, 267 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:52:19,875 : INFO : EPOCH 10 - PROGRESS: at 35.33% examples, 513 words/s, in_qsize 4, out_qsize 0\n",
      "2020-05-28 17:52:43,809 : INFO : EPOCH 10 - PROGRESS: at 50.67% examples, 417 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:52:45,678 : INFO : EPOCH 10 - PROGRESS: at 66.67% examples, 533 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:52:45,686 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:52:47,063 : INFO : EPOCH 10 - PROGRESS: at 83.67% examples, 648 words/s, in_qsize 1, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-28 17:52:47,063 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:52:52,696 : INFO : EPOCH 10 - PROGRESS: at 100.00% examples, 701 words/s, in_qsize 0, out_qsize 1\n",
      "2020-05-28 17:52:52,700 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:52:52,700 : INFO : EPOCH - 10 : training on 58152 raw words (42417 effective words) took 60.5s, 701 effective words/s\n",
      "2020-05-28 17:53:25,296 : INFO : EPOCH 11 - PROGRESS: at 18.67% examples, 219 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:53:27,155 : INFO : EPOCH 11 - PROGRESS: at 34.00% examples, 421 words/s, in_qsize 4, out_qsize 0\n",
      "2020-05-28 17:53:31,239 : INFO : EPOCH 11 - PROGRESS: at 50.67% examples, 561 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:53:59,003 : INFO : EPOCH 11 - PROGRESS: at 66.67% examples, 431 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:53:59,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:54:00,842 : INFO : EPOCH 11 - PROGRESS: at 83.67% examples, 523 words/s, in_qsize 1, out_qsize 1\n",
      "2020-05-28 17:54:00,843 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:54:01,853 : INFO : EPOCH 11 - PROGRESS: at 100.00% examples, 614 words/s, in_qsize 0, out_qsize 1\n",
      "2020-05-28 17:54:01,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:54:01,853 : INFO : EPOCH - 11 : training on 58152 raw words (42437 effective words) took 69.2s, 614 effective words/s\n",
      "2020-05-28 17:54:35,449 : INFO : EPOCH 12 - PROGRESS: at 16.67% examples, 209 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:54:36,838 : INFO : EPOCH 12 - PROGRESS: at 35.33% examples, 406 words/s, in_qsize 4, out_qsize 0\n",
      "2020-05-28 17:54:43,911 : INFO : EPOCH 12 - PROGRESS: at 50.67% examples, 514 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:55:11,767 : INFO : EPOCH 12 - PROGRESS: at 66.67% examples, 409 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:55:11,782 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:55:13,064 : INFO : EPOCH 12 - PROGRESS: at 83.67% examples, 500 words/s, in_qsize 1, out_qsize 1\n",
      "2020-05-28 17:55:13,069 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:55:14,315 : INFO : EPOCH 12 - PROGRESS: at 100.00% examples, 585 words/s, in_qsize 0, out_qsize 1\n",
      "2020-05-28 17:55:14,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:55:14,321 : INFO : EPOCH - 12 : training on 58152 raw words (42391 effective words) took 72.4s, 585 effective words/s\n",
      "2020-05-28 17:55:49,732 : INFO : EPOCH 13 - PROGRESS: at 16.67% examples, 199 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:56:24,051 : INFO : EPOCH 13 - PROGRESS: at 67.00% examples, 407 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:56:24,051 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:56:24,915 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:56:24,924 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:56:24,929 : INFO : EPOCH - 13 : training on 58152 raw words (42381 effective words) took 70.6s, 600 effective words/s\n",
      "2020-05-28 17:56:59,789 : INFO : EPOCH 14 - PROGRESS: at 16.67% examples, 202 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:57:02,207 : INFO : EPOCH 14 - PROGRESS: at 50.67% examples, 579 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:57:35,589 : INFO : EPOCH 14 - PROGRESS: at 66.67% examples, 404 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:57:35,594 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:57:36,848 : INFO : EPOCH 14 - PROGRESS: at 83.67% examples, 495 words/s, in_qsize 1, out_qsize 1\n",
      "2020-05-28 17:57:36,850 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:57:36,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:57:36,919 : INFO : EPOCH - 14 : training on 58152 raw words (42417 effective words) took 72.0s, 589 effective words/s\n",
      "2020-05-28 17:58:16,386 : INFO : EPOCH 15 - PROGRESS: at 16.67% examples, 178 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-28 17:58:18,113 : INFO : EPOCH 15 - PROGRESS: at 35.33% examples, 346 words/s, in_qsize 4, out_qsize 0\n",
      "2020-05-28 17:58:19,864 : INFO : EPOCH 15 - PROGRESS: at 50.67% examples, 502 words/s, in_qsize 3, out_qsize 0\n",
      "2020-05-28 17:58:54,231 : INFO : EPOCH 15 - PROGRESS: at 66.67% examples, 369 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-28 17:58:54,247 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-28 17:58:55,244 : INFO : EPOCH 15 - PROGRESS: at 83.67% examples, 454 words/s, in_qsize 1, out_qsize 1\n",
      "2020-05-28 17:58:55,250 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-28 17:58:55,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-28 17:58:55,551 : INFO : EPOCH - 15 : training on 58152 raw words (42419 effective words) took 78.6s, 540 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "\n",
    "for sentence in cleaned_sentences:\n",
    "    words = sentence.split()\n",
    "    vectors.append(model.infer_vector(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = MeanShift().fit(vectors)\n",
    "labels = clusters.labels_\n",
    "centers = clusters.labels = clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_labels = []\n",
    "\n",
    "for i in range(len(centers)):\n",
    "    indeces = np.where(labels == i)\n",
    "    most_centered = indeces[0][0]\n",
    "    if len(indeces[0]) == 1:\n",
    "        keep_labels.append(most_centered)\n",
    "    else:\n",
    "        for j in range(len(indeces)):\n",
    "            shortest_distance = inf\n",
    "            distance = euclidean(centers[labels[i]], vectors[indeces[0][j]])\n",
    "            if distance < shortest_distance:\n",
    "                shortest_distance = distance\n",
    "                most_centered = indeces[0][j]\n",
    "        keep_labels.append(most_centered)\n",
    "        \n",
    "keep_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_sentences = []\n",
    "\n",
    "for i in range(len(cleaned_sentences)):\n",
    "    if i in keep_labels:\n",
    "        keep_sentences.append(cleaned_sentences[i])\n",
    "\n",
    "summarized_text = ' '.join(sentence for sentence in keep_sentences)   \n",
    "\n",
    "summarized_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
